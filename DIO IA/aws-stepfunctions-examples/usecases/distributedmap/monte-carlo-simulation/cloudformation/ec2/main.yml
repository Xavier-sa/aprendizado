---
AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31
Description: "Creates a Stack using Fargate"

##############################################################
#
# PARAMETERS
#
##############################################################
Parameters:
  Prefix:
    Type: String
    Default: sfn-ec2spot
    Description: This prefix will be prepended to all resource names

  VPCCidr:
    Type: String
    Default: 10.10.0.0/16
    Description: The root range for the VPC

  VPCDomain:
    Type: String
    Default: sfnecs.demo
    Description: The root domain for the VPC
  
  RecordCount:
    Type: Number
    Default: 500000
    Description: Sets how many records will be generated

  BatchSize:
    Type: Number
    Default: 100
    Description: Sets the size of each batch for DMap
  
  ProcessConcurrency:
    Type: Number
    Default: 1000
    Description: Sets the max concurrency 
  
  FedRate:
    Type: Number
    Default: 8
    Description: The fictitious federate rate
  
  ProcessContainerImage:
    Type: String
    Default: public.ecr.aws/amazonlinux/amazonlinux:2023
    Description: Default container image used by the processor

Resources:
##############################################################
#
# VPC
#
##############################################################
  MainVPC:
    Type: AWS::EC2::VPC
    Properties: 
      CidrBlock: !Ref VPCCidr
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'vpc', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# VPC FLOW LOGS
#
##############################################################
  MainFlowLog:
    Type: AWS::EC2::FlowLog
    Properties:
      DeliverLogsPermissionArn: !GetAtt FlowLogsRole.Arn
      LogGroupName: !Ref FlowLogsLogGroup
      ResourceId: !Ref MainVPC
      ResourceType: VPC
      TrafficType: ALL
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'vpc-flowlogs', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  FlowLogsLogGroup: 
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Join ['-', [!Ref Prefix, 'vpc-flowlogs', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'vpc-flowlogs', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  FlowLogsPolicy:
    Type: AWS::IAM::RolePolicy
    Properties:
      PolicyName: !Join ['-', [!Ref Prefix, 'vpc-flowlogs-policy', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      RoleName: !Ref FlowLogsRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogGroups
              - logs:DescribeLogStreams
            Resource: '*'
  
  FlowLogsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "",
              "Effect": "Allow",
              "Principal": {
                "Service": "vpc-flow-logs.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
      Description: VPC Flow Logs Role
      RoleName: !Join ['-', [!Ref Prefix, 'vpc-flowlogs-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'vpc-flowlogs-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# VPC ENDPOINTS
#
##############################################################
  VPCEndpointSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Default group for VPC Endpoints
      SecurityGroupEgress:
        - CidrIp: 0.0.0.0/0
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
      SecurityGroupIngress:
        - CidrIp: !Ref VPCCidr
          IpProtocol: "-1"
          FromPort: 443
          ToPort: 443
      VpcId: !Ref MainVPC
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'vpc-endpoints', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  S3GatewayEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Gateway
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      RouteTableIds:
        - !Ref PublicRouteTable
        - !Ref PrimaryPrivateRouteTable
        - !Ref SecondaryPrivateRouteTable

  ALBInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.elasticloadbalancing'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  ECRInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ecr.dkr'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  ECRAPIInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ecr.api'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  ECSInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ecs'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  ECSAgentInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ecs-agent'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  ECSTelemetryInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ecs-telemetry'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  SSMInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ssm'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  SSMMessagesInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ssmmessages'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

  EC2MessagesInterfaceEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcEndpointType: Interface
      VpcId: !Ref MainVPC
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.ec2messages'
      SubnetIds:
        - !Ref PrimaryPrivateSubnet
        - !Ref SecondaryPrivateSubnet
      PrivateDnsEnabled: true

##############################################################
#
# DHCP OPTIONS
#
##############################################################
  MainDhcpOptions: 
    Type: AWS::EC2::DHCPOptions
    Properties: 
      DomainName: !Ref VPCDomain
      DomainNameServers: 
        - AmazonProvidedDNS
      NtpServers: 
        - 129.6.15.29
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'dhcp', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  MainDHCPOptionsAssociation:
    Type: AWS::EC2::VPCDHCPOptionsAssociation
    Properties:
      VpcId: !Ref MainVPC
      DhcpOptionsId: !Ref MainDhcpOptions

##############################################################
#
# SUBNETS
#
##############################################################
  PrimaryPublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MainVPC
      CidrBlock: !Select [ 0, !Cidr [ !GetAtt MainVPC.CidrBlock, 4, 12 ]]
      AvailabilityZone: !Join ['', [!Ref AWS::Region, 'a']]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'ppub', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  SecondaryPublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MainVPC
      CidrBlock: !Select [ 1, !Cidr [ !GetAtt MainVPC.CidrBlock, 4, 12 ]]
      AvailabilityZone: !Join ['', [!Ref AWS::Region, 'c']]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'spub', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  PrimaryPrivateSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MainVPC
      CidrBlock: !Select [ 2, !Cidr [ !GetAtt MainVPC.CidrBlock, 4, 12 ]]
      AvailabilityZone: !Join ['', [!Ref AWS::Region, 'a']]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'pprv', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  SecondaryPrivateSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref MainVPC
      CidrBlock: !Select [ 3, !Cidr [ !GetAtt MainVPC.CidrBlock, 4, 12 ]]
      AvailabilityZone: !Join ['', [!Ref AWS::Region, 'c']]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'sprv', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# ROUTING
#
##############################################################
  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'igw', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  PrimaryNATGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt PrimaryNATGatewayEIP.AllocationId
      SubnetId: !Ref PrimaryPublicSubnet
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'pnat', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  PrimaryNATGatewayEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'pnat', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  SecondaryNATGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt SecondaryNATGatewayEIP.AllocationId
      SubnetId: !Ref SecondaryPublicSubnet
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'snat', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  SecondaryNATGatewayEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'snat', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref MainVPC
      InternetGatewayId: !Ref InternetGateway

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref MainVPC
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'public', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  PrimaryPrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref MainVPC
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'pprivate', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  SecondaryPrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref MainVPC
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'sprivate', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  DefaultPublicRoute:
    Type: AWS::EC2::Route
    Properties:
       RouteTableId: !Ref PublicRouteTable
       DestinationCidrBlock: 0.0.0.0/0
       GatewayId: !Ref InternetGateway

  PrimaryPrivateRoute:
    DependsOn: PrimaryNATGateway
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrimaryPrivateRouteTable
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref PrimaryNATGateway

  SecondaryPrivateRoute:
    DependsOn: SecondaryNATGateway
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref SecondaryPrivateRouteTable
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref SecondaryNATGateway

  PrimaryPublicSubnetAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrimaryPublicSubnet
      RouteTableId: !Ref PublicRouteTable

  SecondaryPublicSubnetAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref SecondaryPublicSubnet
      RouteTableId: !Ref PublicRouteTable

  PrimaryPrivateSubnetAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrimaryPrivateSubnet
      RouteTableId: !Ref PrimaryPrivateRouteTable

  SecondaryPrivateSubnetAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref SecondaryPrivateSubnet
      RouteTableId: !Ref SecondaryPrivateRouteTable

##############################################################
#
# SECURITY GROUPS
#
##############################################################
  ECSInstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Applied to EC2 instances to designtate being part of an ECS Cluster
      SecurityGroupEgress:
        - CidrIp: 0.0.0.0/0
          IpProtocol: "-1"
          FromPort: 0
          ToPort: 0
      VpcId: !Ref MainVPC
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'ecs-instance', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# S3 BUCKETS
#
##############################################################
  SourceS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Join ['-', [!Ref Prefix, 'source', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'source', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  DestinationS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Join ['-', [!Ref Prefix, 'destination', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'destination', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# LAMBDA IAM ROLE
#
##############################################################
  LambdaPolicy:
    Type: AWS::IAM::RolePolicy
    Properties:
      PolicyName: !Join ['-', [!Ref Prefix, 'lambda-policy', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      RoleName: !Ref LambdaRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - states:StartExecution
              - states:DescribeExecution
              - states:StopExecution
            Resource: '*'
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogGroups
              - logs:DescribeLogStreams
            Resource: '*'
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
            Resource:
              - !Join ['', [!GetAtt SourceS3Bucket.Arn, '*']]
              - !Join ['', [!GetAtt DestinationS3Bucket.Arn, '*']]

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "",
              "Effect": "Allow",
              "Principal": {
                "Service": "lambda.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
      Description: Lambda Role reused by all Lambda Functions in Stack
      RoleName: !Join ['-', [!Ref Prefix, 'lambda-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'lambda-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# LAMBDA FUNCTIONS
#
##############################################################
  FunctionECSRoleCreate:
    Type: 'AWS::Serverless::Function'
    Properties:
      InlineCode: |
        /* Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
        * SPDX-License-Identifier: MIT-0 */

        // A Lambda function that looks up the ECS Service linked role and creates it if it does not exist.
        var response = require('./cfn-response.js');

        function sleep(ms) {
          return new Promise((resolve) => {
            setTimeout(resolve, ms);
          });
        }

        function send(event, context, responseStatus, responseData, physicalResourceId, noEcho) {
            try {
              const https = require("https");
              const { URL } = require("url");

              const responseBody = {
                Status: responseStatus,
                Reason: "See the details in CloudWatch Log Stream: " + context.logStreamName,
                PhysicalResourceId: context.logStreamName,
                StackId: event.StackId,
                RequestId: event.RequestId,
                LogicalResourceId: event.LogicalResourceId,
                NoEcho: false,
                Data: responseData,
              };
              console.log("Response body:\n", JSON.stringify(responseBody));

              const parsedUrl = new URL(event.ResponseURL);
              const requestOptions = {
                hostname: parsedUrl.hostname,
                port: 443,
                path: parsedUrl.pathname + parsedUrl.search,
                method: "PUT",
                headers: {
                  "content-type": "",
                  "content-length": JSON.stringify(responseBody).length,
                },
              };
              console.log("Request options:\n", JSON.stringify(requestOptions));

              // Send response back to CloudFormation
              return new Promise((resolve, reject) => {
                const request = https.request(requestOptions, function (response) {
                  console.log("Status code: ", response.statusCode);
                  response.on("data", () => {});
                  response.on("end", () => {
                    console.log("Status code: ", response.statusCode);
                    console.log("Status message: ", response.statusMessage);
                    resolve("Success");
                  });
                });
                request.on("error", (e) => {
                  console.error(e);
                  reject("Error");
                });
                request.write(JSON.stringify(responseBody));
                request.end();
              });
            } catch (error) {
              console.error("Error in cfn_response:\n", error);
              return;
            }
          };

        exports.handler = async function(event, context) {
            const { IAMClient, CreateServiceLinkedRoleCommand, GetRoleCommand, NoSuchEntityException } = require("@aws-sdk/client-iam");
            const iamClient = new IAMClient();
            var responseStatus = response.SUCCESS
            var responseData = {};
            if (event.RequestType == "Delete") {
                await send(event, context, responseStatus, responseData);
                return;
            };

            var params = {
              AWSServiceName: 'ecs.amazonaws.com', /* required */
              Description: 'ECS Service Linked Role'
            };
            try {
              const command = new CreateServiceLinkedRoleCommand(params);
              responseData = await iamClient.send(command);
            } catch (createError) {
              // Just logging the error as it does not matter if the role already exists.
              responseData = createError
            }
            await sleep(5000)
            console.log(responseData);
            await send(event, context, responseStatus, responseData);
            return;
        };
      Runtime: nodejs18.x
      MemorySize: 1024
      Timeout: 300
      Handler: index.handler
      Policies:
        - Statement:
          - Sid: CreateECSServiceLinkedRole
            Effect: Allow
            Action: iam:CreateServiceLinkedRole
            Resource: "arn:aws:iam::*:role/aws-service-role/ecs.amazonaws.com/AWSServiceRoleForECS*"
            Condition:
              StringLike:
                iam:AWSServiceName: ecs.amazonaws.com
          - Sid: AttachECSServiceLinkPolicy
            Effect: Allow
            Action:
              - iam:PassRole
              - iam:PutRolePolicy
              - iam:GetRole
            Resource: "arn:aws:iam::*:role/aws-service-role/ecs.amazonaws.com/AWSServiceRoleForECS*"

  CustomResourceECSRoleCreate:
    Type: 'Custom::ECSRoleCreate'
    Properties:
      ServiceToken: !GetAtt FunctionECSRoleCreate.Arn

  SeedFileGenerationLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'seedgen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 180
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          RECORDCOUNT: !Ref RecordCount
      Code:
        ZipFile: |
          import boto3
          import csv
          from io import StringIO
          import os

          region = os.getenv('REGION')
          count = os.getenv('RECORDCOUNT')
          s3_client = boto3.client('s3', region_name=region)
          s3 = boto3.resource('s3')

          def lambda_handler(event, context):
            data = []
            for i in range(1, (int(count) + 1)):
              data.append({
                'num': i
              })

            stream = StringIO()
            headers = list(data[0].keys())
            writer = csv.DictWriter(stream, fieldnames=headers)
            writer.writeheader()
            writer.writerows(data)
            body = stream.getvalue()

            dst = s3.Object(event['bucket'], 'inventory/numbers.csv')
            dst.put(Body=body)
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'seedgen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  DataGenerationLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'datagen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 900
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          RECORDCOUNT: !Ref RecordCount
      Code:
        ZipFile: |
          import boto3
          import csv
          from random import uniform, randrange, randint
          from datetime import datetime, timedelta
          from io import StringIO
          import os

          region = os.getenv('REGION')
          count = os.getenv('RECORDCOUNT')
          s3_client = boto3.client('s3', region_name=region)
          s3 = boto3.resource('s3')
          terms = [12, 24, 36, 48, 60, 72, 84, 96, 108, 120]
          term = terms[randint(0,9)]
          end = datetime.now()
          start = end - timedelta(days=((term / 12) * 365))

          def random_date(start, end):
            delta = end - start
            int_delta = (delta.days * 24 * 60 * 60) + delta.seconds
            random_second = randrange(int_delta)
            return start + timedelta(seconds=random_second)
        
          def get_zeroes(current, total):
            x = total - len(str(current))
            ret = ""
            for i in range(0, x):
              ret = ret + "0"
            return ret + str(current)

          def lambda_handler(event, context):
            batch = []
            length = len(str(count))
            first = get_zeroes(event['Items'][0]['num'], length)
            last = get_zeroes(event['Items'][len(event['Items']) - 1]['num'], length)
            for item in event['Items']:
              data = [{
                'AccountID': randint(1000000,9999999),
                'ZipCode': randint(10000,99999),
                'Rate': round(uniform(1.0,9.9), 2),
                'Payment': randint(1000,10000),
                'LoanAmount': randint(10000,10000000),
                'LoanTerm': term,
                'OriginationDate': random_date(start, end).strftime('%m/%d/%Y'),
                'GrossIncome': randint(50000,5000000)
              }]

              stream = StringIO()
              headers = list(data[0].keys())
              writer = csv.DictWriter(stream, fieldnames=headers)
              writer.writeheader()
              writer.writerows(data)
              body = stream.getvalue()

              number = get_zeroes(item['num'], length)

              dst = s3.Object(event['BatchInput']['bucket'], 'data/data-gen-' + str(number) + '.csv')
              dst.put(Body=body)

              batch.append({
                'Key': 'data/data-gen-' + number + '.csv',
                'Size': len(body.encode('utf-8'))
              })
            
            stream = StringIO()
            headers = list(batch[0].keys())
            writer = csv.DictWriter(stream, fieldnames=headers)
            writer.writeheader()
            writer.writerows(batch)
            body = stream.getvalue()

            dst = s3.Object(event['BatchInput']['bucket'], 'temp/data-batch-' + str(first) + '-' + str(last) + '.csv')
            dst.put(Body=body)
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'datagen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  InventoryLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'inventory', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 300
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          RECORDCOUNT: !Ref RecordCount
      Code:
        ZipFile: |
          import boto3
          import csv
          import gzip
          from io import StringIO
          from io import BytesIO
          from botocore.client import Config
          import os

          # set a few variables we'll use to get our data
          region = os.getenv('REGION')
          count = os.getenv('RECORDCOUNT')
          s3_client = boto3.client('s3', region_name=region)
          s3 = boto3.resource('s3')
        
          def get_zeroes(current, total):
            x = total - len(str(current))
            ret = ""
            for i in range(0, x):
              ret = ret + "0"
            return ret + str(current)

          def lambda_handler(event, context):
            data = []
            length = len(str(count))
            start = 0
            end = 0
            for x in range(0, len(event['Items'])):
              source = s3_client.get_object(Bucket=event['BatchInput']['bucket'], Key=event['Items'][x]['Key'])
              content = source.get('Body').read().decode('utf-8')
              buf = StringIO(content)
              reader = csv.DictReader(buf)
              objects = list(reader)
            
              for item in objects:
                start = int(item['Key'].split('-')[2].replace('.csv','')) if int(item['Key'].split('-')[2].replace('.csv','')) < int(start) or int(start) == 0 else int(start)
                end = int(item['Key'].split('-')[2].replace('.csv','')) if int(item['Key'].split('-')[2].replace('.csv','')) > int(end) else int(end)
                data.append({
                  'Bucket': event['BatchInput']['bucket'],
                  'Key': item['Key'],
                  'Size': item['Size']
                })

            mem = BytesIO()
            with gzip.GzipFile(fileobj=mem, mode='w') as gz:
              stream = StringIO()
              headers = list(data[0].keys())
              writer = csv.DictWriter(stream, fieldnames=headers)
              writer.writerows(data)

              gz.write(stream.getvalue().encode())
              gz.close()
              mem.seek(0)

            s3_client.upload_fileobj(Fileobj=mem, Bucket=event['BatchInput']['bucket'], Key='inventory/data-gen-' + str(get_zeroes(start, length)) + '-' + str(get_zeroes(end, length)) + '.csv.gz')
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'inventory', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  ManifestLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'manifest', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 180
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          RECORDCOUNT: !Ref RecordCount
      Code:
        ZipFile: |
          import boto3
          import json
          from datetime import datetime
          import time

          s3_client = boto3.client("s3")
          s3 = boto3.resource("s3")

          def lambda_handler(event, context):
            files = []
            for item in event['Items']:
              files.append({
                "key": item['Key'],
                "size": item['Size'],
                "MD5checksum": item['Etag'].replace('"','')
              })
            manifest = {
              "sourceBucket" : event['BatchInput']['bucket'],
              "destinationBucket" : "arn:aws:s3:::" + event['BatchInput']['bucket'],
              "version" : "2016-11-30",
              "creationTimestamp" : time.mktime(datetime.now().timetuple()),
              "fileFormat" : "CSV",
              "fileSchema" : "Bucket, Key, Size",
              "files" : files
            }
            dst = s3.Object(event['BatchInput']['bucket'], 'inventory/manifest.json')
            dst.put(Body=(bytes(json.dumps(manifest).encode('UTF-8'))))
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'manifest', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  InventoryPartitionLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'inventorypartition', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Layers:
        - !Join [':', ['arn:aws:lambda', !Ref AWS::Region, '336392948345', 'layer', 'AWSSDKPandas-Python310:5']]
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 512
      Timeout: 180
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          RECORDCOUNT: !Ref RecordCount
          SOURCEBUCKET: !Ref SourceS3Bucket
      Code:
        ZipFile: |
          import boto3
          import json
          import csv
          import pandas as pd
          import io
          import os

          region = os.getenv('REGION')
          count = os.getenv('RECORDCOUNT')
          s3_client = boto3.client('s3', region_name=region)
          s3_resource = boto3.resource('s3')

          def lambda_handler(event, context):
            bucket_v = event['inventory']['bucket']
            manifest_key_v = event['inventory']['key']
            new_manifest_key_prefix = event['inventory']['output_prefix']
            input_sampling = event['workshop_variables']['input_sampling']
            original_manifest = s3_client.get_object(Bucket=bucket_v, Key=manifest_key_v)
            original_manifest_json = json.loads(original_manifest['Body'].read())
            print(original_manifest_json)
            bucket = s3_resource.Bucket(bucket_v)
            df_batch_inventory = pd.DataFrame()
            output_manifest_manifest = {
              'files': []
            }
            output_manifest_manifest['bucket'] = bucket_v 
            
            # Record Counting Variables
            total_records = 0
            output_records = 0
            
            #If not sampling the input (sampling = 1) then we can just re-write manifest.json files only
            manifest_counter = 1
            if input_sampling == 1:
              for file in original_manifest_json['files']:
                inventory_manifest = {
                  'files': []
                }
                inventory_manifest['sourceBucket'] = original_manifest_json['sourceBucket']
                inventory_manifest['destinationBucket'] = original_manifest_json['destinationBucket']
                inventory_manifest['fileFormat'] = original_manifest_json['fileFormat']
                inventory_manifest['fileSchema'] = original_manifest_json['fileSchema']
                inventory_manifest['files'].append({
                  'key': file['key'],
                  'size': file['size']
                })
                inventory_manifest_json = json.dumps(inventory_manifest)
                s3_resource.Object(bucket_v, new_manifest_key_prefix + 'manifest--{}.json'.format(manifest_counter)).put(Body=inventory_manifest_json)
                output_manifest_manifest['files'].append({
                  'key': new_manifest_key_prefix + 'manifest--{}.json'.format(manifest_counter),
                  'bucket': bucket_v
                })
                manifest_counter += 1
            #If sampling or filtering the input dataset we will read and process the inventory CVS's and create modified versions for processing        
            else:
              im = 1
              i_files = 1
              for file in original_manifest_json['files']:
                obj = s3_resource.Object(bucket_v,file['key'])
                print(obj.key)
                obj_data = io.BytesIO(obj.get()['Body'].read())
                # if file['key'] contains .gz then we are reading the .gz file and not the .csv file
                if '.gz' in file['key']:
                  df_temp = pd.read_csv(obj_data, compression='gzip', names=['Bucket', 'Key', 'Size'], header=None)
                else:
                  df_temp = pd.read_csv(obj_data, names=['Bucket', 'Key', 'Size'], header=None)
                total_records += len(df_temp)
                print("Current observed record count: " + format(total_records))
                df_batch_inventory = pd.concat([df_batch_inventory,df_temp])
                if (len(df_batch_inventory) > 250000) or i_files == len(original_manifest_json['files']):
                  inventory_manifest = {
                    'files': []
                  }
                  inventory_manifest['sourceBucket'] = original_manifest_json['sourceBucket']
                  inventory_manifest['destinationBucket'] = original_manifest_json['destinationBucket']
                  inventory_manifest['fileFormat'] = original_manifest_json['fileFormat']
                  inventory_manifest['fileSchema'] = original_manifest_json['fileSchema']
                  df_batch_inventory = df_batch_inventory[::input_sampling]
                  csv_buffer = io.StringIO()
                  output_records += len(df_batch_inventory)
                  print("Output records this batch: " + format(len(df_batch_inventory)))
                  print("Total output records to this point: " + format(output_records))
                  df_batch_inventory.to_csv(csv_buffer, index=False, header=False)
                  csv_tmp_name = new_manifest_key_prefix + 'inventory-' + format(im) + '.csv'
                  s3_resource.Object(bucket_v, csv_tmp_name.format(im)).put(Body=csv_buffer.getvalue())
                  inventory_manifest['files'].append({
                    'key': csv_tmp_name,
                    'size': len(csv_buffer.getvalue())
                  })
                  print(inventory_manifest)
                  inventory_manifest_json = json.dumps(inventory_manifest)
                  s3_resource.Object(bucket_v, new_manifest_key_prefix + 'manifest--{}.json'.format(im)).put(Body=inventory_manifest_json)
                  output_manifest_manifest['files'].append({
                    'key': new_manifest_key_prefix + 'manifest--{}.json'.format(im),
                    'bucket': bucket_v
                  })
                  im += 1
                  df_batch_inventory = pd.DataFrame(None)
                i_files += 1
            return {
              'statusCode': 200,
              'body': output_manifest_manifest
            }
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'inventorypartition', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  ScriptFeederLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Join ['-', [!Ref Prefix, 'scriptfeeder', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Runtime: python3.10
      Role: !GetAtt LambdaRole.Arn
      Handler: index.lambda_handler
      MemorySize: 256
      Timeout: 180
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          SOURCEBUCKET: !Ref SourceS3Bucket
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import os

          # set a few variables we'll use to get our data
          region = os.getenv('REGION')
          bucket = os.getenv('SOURCEBUCKET')

          s3_client = boto3.client('s3', region_name=region)
          s3 = boto3.resource('s3')

          def lambda_handler(event, context):
            body = """#!/usr/bin/python3
          import boto3
          import botocore
          import os
          import json
          import pandas as pd
          from io import StringIO
          from botocore.config import Config
          from random import randint

          # set initial variables for the rest of the script
          config = Config(
            connect_timeout=65,
            read_timeout=65,
            retries={'max_attempts': 0}
          )
          region = os.getenv('REGION')
          count = os.getenv('RECORDCOUNT')
          fedrate = os.getenv('FEDRATE')
          client = boto3.client('stepfunctions', region_name=region, config=config)
          s3 = boto3.client('s3', region_name=region)
          s3_resource = boto3.resource('s3')

          # set a few variables we'll use to get our data
          activity_arn = os.getenv('ACTIVITY_ARN')
          worker_name = os.getenv('HOSTNAME')

          # this function is simulating calculating the percentage of likelihood
          # a given loan would default based on a given federal rate and the existing
          # rate and payment of the loan. for simplicity we are simply returning
          # a random percentage    
          def calculate_default(df, fedrate):
            df['WillDefault'] = (randint(1,100)/100)
            return df

          # you always need a little error handling :) in this case what we
          # are specifically looking for is SlowDown errors from S3. when that 
          # occurs we are raising the error to let Step Functions handle it
          def handle_processing_errors(error):
            print("botocore Error Caught")
            if error.response['Error']['Code'] == 'SlowDown':
              print ("Client SlowDown Error")
              # Throw 503 from S3
              class SlowDown(Exception):
                pass
              raise SlowDown('Reduce S3 Requests')

          # this function just prepends zeroes to the object names
          # to make it prettier and easier to read
          def get_zeroes(current, total):
            x = total - len(str(current))
            ret = ""
            for i in range(0, x):
              ret = ret + "0"
            return ret + str(current)

          # this function finds the lowest value in the list of object names
          def get_start(id,start):
            ret = id if id < int(start) or int(start) == 0 else int(start)
            return ret

          # this function finds the highest value in the list of object names
          def get_end(id,end):
            ret = id if id > int(end) else int(end)
            return ret

          # this functions writes to s3
          def write_output(bucket, batch, rowsper, prefix, start, end):
            # write the data
            buffer = StringIO()
            batch.to_csv(buffer, index=False)
            key = prefix + "/data-gen-" + get_zeroes(start, len(count)) + ".csv" if rowsper == 1 else prefix + "/data-gen-batch-" + get_zeroes(start, len(count)) + "_" + get_zeroes(end, len(count)) + ".csv"

            # write the object
            try:
              s3_resource.Object(bucket, key).put(Body=buffer.getvalue())
            except botocore.exceptions.ClientError as error:
              handle_processing_errors(error)

          # now we start polling until we have nothing left to do. i realize this should
          # be more functions and it's pretty gross but it works for a demo :) 
          while True:
            response = client.get_activity_task(
              activityArn = activity_arn,
              workerName = worker_name
            )

            if 'input' not in response.keys() or 'taskToken' not in response.keys():
              print('no tasks to process...waiting 30 seconds to try again')
              time.sleep(30)
              continue

            # setup variables to be used throughout the script
            token = response['taskToken']
            event = json.loads(response['input'])
            success = True
            cause = ""
            error = ""
            start = 0
            end = 0
            counter = 0
            x = 0

            # set variables passed from the Set Variables step of the Step Function workflow
            prefix = event['BatchInput']['workshop_variables']['output_prefix']
            bucket = event['BatchInput']['workshop_variables']['output_bucket']
            rowsper = event['BatchInput']['workshop_variables']['output_rows_per_file']
            
            # instantiate the initial dataframe
            batch = pd.DataFrame()

            # load the full batch from Step Functions into the dataframe
            for item in event['Items']:
              # our s3 inventory report may contain objects we don't care about. this
              # conditional will ensure we only process source CSV Files, 
              # skipping folders an other metadata object entries.
              if str(item['Key']).find(".csv") == -1: continue

              # increment our counter
              counter+=1
              x+=1

              # with that out of the way lets get our data
              try:
                source = s3.get_object(Bucket=item['Bucket'], Key=item['Key'])
                content = source['Body'].read().decode('utf-8')
              except botocore.exceptions.ClientError as error:
                handle_processing_errors(error)
              
              # with data in hand we can load the content into a dataframe
              df = pd.read_csv(StringIO(content))

              # calculate our percentage for defaulting
              df = calculate_default(df, fedrate)

              # and finally add it to our batch of dataframes 
              batch = pd.concat([batch,df])

              # our file names will include the first and last object
              # id to make them easier to find. 
              id = int(item['Key'].split('-')[2].replace('.csv','')) 
              start = get_start(id, start)
              end = get_end(id, end)
              
              # do we need to write yet? we'll check to see we hit that yet
              if counter == rowsper or x >= len(event['Items']):
                # write the object(s)
                write_output(bucket, batch, rowsper, prefix, str(start), str(end))

                # rinse and repeat :)
                start = 0
                end = 0
                counter = 0
                batch.empty
            
            client.send_task_success(
              taskToken = token,
              output = "{\\"message\\": \\"success\\"}"
            )"""
            
            dst = s3.Object(bucket, 'script/process.py')
            dst.put(Body=body)
            responseValue = 0
            responseData = {}
            responseData['Data'] = responseValue
            cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'scriptfeeder', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
  
  ScriptFeederLambdaInvoke:
    Type: AWS::CloudFormation::CustomResource
    DependsOn: ScriptFeederLambda
    Version: "1.0"
    Properties:
      ServiceToken: !GetAtt ScriptFeederLambda.Arn

##############################################################
#
# ECS LOG GROUP
#
##############################################################
  ECSServiceLogGroup: 
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# ECS CLUSTER
#
##############################################################
  MainECSCluster:
    Type: AWS::ECS::Cluster
    DependsOn:
      - ECSExecPolicy
      - CustomResourceECSRoleCreate
    Properties:
      ClusterName: !Join ['-', [!Ref Prefix, 'ecs-cluster', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      ClusterSettings:
        - Name: containerInsights
          Value: enabled
      CapacityProviders:
        - FARGATE_SPOT
      DefaultCapacityProviderStrategy:
        - Base: 1
          Weight: 100
          CapacityProvider: FARGATE_SPOT
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'ecs-cluster', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# ECS SECURITY GROUPS
#
##############################################################
  ProcessTaskSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties: 
      GroupDescription: Allows access to VPC
      GroupName: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      SecurityGroupEgress:
        - IpProtocol: -1
          FromPort: 0
          ToPort: 0
          CidrIp: 0.0.0.0/0
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      VpcId: !Ref MainVPC

##############################################################
#
# ECS IAM ROLES
#
##############################################################
  ECSTaskPolicy:
    Type: AWS::IAM::RolePolicy
    Properties:
      PolicyName: !Join ['-', [!Ref Prefix, 'ecs-task-policy', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      RoleName: !Ref ECSTaskRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogGroups
              - logs:DescribeLogStreams
            Resource: '*'
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
            Resource:
              - !Join ['', [!GetAtt SourceS3Bucket.Arn, '*']]
              - !Join ['', [!GetAtt DestinationS3Bucket.Arn, '*']]
          - Effect: Allow
            Action:
              - states:DescribeActivity
              - states:DeleteActivity
              - states:GetActivityTask
              - states:SendTaskHeartbeat
              - states:SendTaskSuccess
              - states:SendTaskFailure
            Resource:
              - !GetAtt MainSFNActivity.Arn
  
  ECSTaskRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "",
              "Effect": "Allow",
              "Principal": {
                "Service": "ecs-tasks.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
      Description: ECS Task Role
      RoleName: !Join ['-', [!Ref Prefix, 'ecs-task-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'ecs-task-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  ECSExecPolicy:
    Type: AWS::IAM::RolePolicy
    Properties:
      PolicyName: !Join ['-', [!Ref Prefix, 'ecs-exec-policy', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      RoleName: !Ref ECSExecRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - ecr:*
              - s3:*
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogGroups
              - logs:DescribeLogStreams
            Resource: '*'
  
  ECSExecRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "",
              "Effect": "Allow",
              "Principal": {
                "Service": "ecs-tasks.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
      Description: ECS Task Role
      RoleName: !Join ['-', [!Ref Prefix, 'ecs-exec-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'ecs-exec-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# ECS SERVICE
#
##############################################################
  MainService:
    Type: AWS::ECS::Service
    Properties:
      CapacityProviderStrategy:
        - Base: 1
          CapacityProvider: FARGATE_SPOT
          Weight: 100
      Cluster: !Ref MainECSCluster
      DesiredCount: 0
      NetworkConfiguration:
        AwsvpcConfiguration:
          AssignPublicIp: DISABLED
          SecurityGroups:
            - !Ref ProcessTaskSecurityGroup
          Subnets:
            - !Ref PrimaryPrivateSubnet
            - !Ref SecondaryPrivateSubnet
      SchedulingStrategy: REPLICA
      ServiceName: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      TaskDefinition: !Ref MainProcessorTaskDefinition
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# ECS TASK DEFINITION
#
##############################################################
  MainProcessorTaskDefinition: 
    Type: AWS::ECS::TaskDefinition
    Properties:
      ContainerDefinitions: 
        - Command:
            - /bin/sh
            - -c
            - "yum -y update && yum -y install awscli python3-pip && aws s3 cp s3://${SOURCEBUCKET}/script/process.py . && python3 -m pip install boto3 && python3 -m pip install pandas && python3 process.py"
          Cpu: 256
          Environment:
            - Name: REGION
              Value: !Ref AWS::Region
            - Name: ACTIVITY_ARN
              Value: !GetAtt MainSFNActivity.Arn
            - Name: RECORDCOUNT
              Value: !Ref RecordCount
            - Name: SOURCEBUCKET
              Value: !Ref SourceS3Bucket
            - Name: DESTINATIONBUCKET
              Value: !Ref DestinationS3Bucket
          Essential: true
          Image: !Ref ProcessContainerImage
          LogConfiguration:
            LogDriver: awslogs
            Options:
              awslogs-group: !Ref ECSServiceLogGroup
              awslogs-region: !Ref AWS::Region
              awslogs-stream-prefix: !Ref Prefix
          Memory: 512
          Name: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Cpu: 256
      ExecutionRoleArn: !Ref ECSExecRole
      Family: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Memory: 512
      NetworkMode: awsvpc
      RequiresCompatibilities:
        - FARGATE
      TaskRoleArn: !Ref ECSTaskRole
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'process', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# STEP FUNCTIONS IAM ROLE
#
##############################################################
  StepFunctionsPolicy:
    Type: AWS::IAM::RolePolicy
    Properties:
      PolicyName: !Join ['-', [!Ref Prefix, 'sfn-policy', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      RoleName: !Ref StepFunctionsRole
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - states:StartExecution
              - states:DescribeExecution
              - states:StopExecution
            Resource: '*'
          - Effect: Allow
            Action:
              - ecs:UpdateService
              - ecs:GetService
            Resource:
              - !Ref MainService
          - Effect: Allow
            Action:
              - events:PutTargets
              - events:PutRule
              - events:DescribeRule
            Resource: '*'
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogGroups
              - logs:DescribeLogStreams
            Resource: '*'
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
            Resource:
              - !Join ['', [!GetAtt SourceS3Bucket.Arn, '*']]
              - !Join ['', [!GetAtt DestinationS3Bucket.Arn, '*']]
          - Effect: Allow
            Action:
              - lambda:InvokeFunction
            Resource:
              - !Join ['', [!GetAtt SeedFileGenerationLambda.Arn, '*']]
              - !Join ['', [!GetAtt DataGenerationLambda.Arn, '*']]
              - !Join ['', [!GetAtt InventoryLambda.Arn, '*']]
              - !Join ['', [!GetAtt ManifestLambda.Arn, '*']]
              - !Join ['', [!GetAtt InventoryPartitionLambda.Arn, '*']]

  StepFunctionsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "",
              "Effect": "Allow",
              "Principal": {
                "Service": "states.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
      Description: Step Functions Role
      RoleName: !Join ['-', [!Ref Prefix, 'sfn-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'sfn-role', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# STEP FUNCTIONS ACTIVITY
#
##############################################################
  MainSFNActivity:
    Type: AWS::StepFunctions::Activity
    Properties: 
      Name: !Join ['-', [!Ref Prefix, 'activity', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Tags: 
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'activity', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

##############################################################
#
# STEP FUNCTIONS STATE MACHINES
#
##############################################################
  DataGenerationStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Join ['-', [!Ref Prefix, 'datagen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Definition:
        Comment: Data Generation State Machine
        StartAt: Seed File Generation
        States:
          Seed File Generation:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            OutputPath: "$.Payload"
            Parameters:
              Payload:
                bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
              FunctionName: !Join [':', [!GetAtt SeedFileGenerationLambda.Arn, '$LATEST']]
            Retry:
            - ErrorEquals:
              - Lambda.ServiceException
              - Lambda.AWSLambdaException
              - Lambda.SdkClientException
              - Lambda.TooManyRequestsException
              IntervalSeconds: 2
              MaxAttempts: 6
              BackoffRate: 2
            Next: File Generation DMap
          File Generation DMap:
            Type: Map
            ItemProcessor:
              ProcessorConfig:
                Mode: DISTRIBUTED
                ExecutionType: STANDARD
              StartAt: File Generation
              States:
                File Generation:
                  Type: Task
                  Resource: arn:aws:states:::lambda:invoke
                  OutputPath: "$.Payload"
                  Parameters:
                    Payload.$: "$"
                    FunctionName: !Join [':', [!GetAtt DataGenerationLambda.Arn, '$LATEST']]
                  Retry:
                  - ErrorEquals:
                    - Lambda.ServiceException
                    - Lambda.AWSLambdaException
                    - Lambda.SdkClientException
                    - Lambda.TooManyRequestsException
                    IntervalSeconds: 2
                    MaxAttempts: 6
                    BackoffRate: 2
                  End: true
            ItemReader:
              Resource: arn:aws:states:::s3:getObject
              ReaderConfig:
                InputType: CSV
                CSVHeaderLocation: FIRST_ROW
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
                Key: inventory/numbers.csv
            MaxConcurrency: 100
            Label: FileGenerationDMap
            ItemBatcher:
              MaxItemsPerBatch: 1000
              BatchInput:
                bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
            ResultWriter:
              Resource: arn:aws:states:::s3:putObject
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt DestinationS3Bucket.Arn]]
                Prefix: !Join ['-', [!Ref Prefix, 'datagen', 'results']]
            Next: Inventory Generation DMap
          Inventory Generation DMap:
            Type: Map
            ItemProcessor:
              ProcessorConfig:
                Mode: DISTRIBUTED
                ExecutionType: STANDARD
              StartAt: Inventory Generation
              States:
                Inventory Generation:
                  Type: Task
                  Resource: arn:aws:states:::lambda:invoke
                  OutputPath: "$.Payload"
                  Parameters:
                    Payload.$: "$"
                    FunctionName: !Join [':', [!GetAtt InventoryLambda.Arn, '$LATEST']]
                  Retry:
                  - ErrorEquals:
                    - Lambda.ServiceException
                    - Lambda.AWSLambdaException
                    - Lambda.SdkClientException
                    - Lambda.TooManyRequestsException
                    IntervalSeconds: 2
                    MaxAttempts: 6
                    BackoffRate: 2
                  End: true
            ItemReader:
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
                Prefix: "temp/data-batch-"
              Resource: arn:aws:states:::s3:listObjectsV2
            MaxConcurrency: 100
            Label: InventoryGenerationDMap
            ItemBatcher:
              MaxItemsPerBatch: 100
              BatchInput:
                bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
            ResultWriter:
              Resource: arn:aws:states:::s3:putObject
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt DestinationS3Bucket.Arn]]
                Prefix: !Join ['-', [!Ref Prefix, 'inventory', 'results']]
            Next: Manifest Generation DMap
          Manifest Generation DMap:
            Type: Map
            ItemProcessor:
              ProcessorConfig:
                Mode: DISTRIBUTED
                ExecutionType: STANDARD
              StartAt: Manifest Generation
              States:
                Manifest Generation:
                  Type: Task
                  Resource: arn:aws:states:::lambda:invoke
                  OutputPath: "$.Payload"
                  Parameters:
                    Payload.$: "$"
                    FunctionName: !Join [':', [!GetAtt ManifestLambda.Arn, '$LATEST']]
                  Retry:
                  - ErrorEquals:
                    - Lambda.ServiceException
                    - Lambda.AWSLambdaException
                    - Lambda.SdkClientException
                    - Lambda.TooManyRequestsException
                    IntervalSeconds: 2
                    MaxAttempts: 6
                    BackoffRate: 2
                  End: true
            ItemReader:
              Resource: arn:aws:states:::s3:listObjectsV2
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
                Prefix: inventory/data-gen-
            MaxConcurrency: 1
            Label: ManifestGenerationDMap
            ItemBatcher:
              MaxItemsPerBatch: 1000
              BatchInput:
                bucket: !Select [5, !Split [':', !GetAtt SourceS3Bucket.Arn]]
            ResultWriter:
              Resource: arn:aws:states:::s3:putObject
              Parameters:
                Bucket: !Select [5, !Split [':', !GetAtt DestinationS3Bucket.Arn]]
                Prefix: !Join ['-', [!Ref Prefix, 'manifest', 'results']]
            End: true
      RoleArn: !GetAtt StepFunctionsRole.Arn
      Tags:
        - Key: Name
          Value: !Join ['-', [!Ref Prefix, 'datagen', !Select [7, !Split ['-', !Ref AWS::StackId]]]]

  DataProcessorStateMachine:
    Type: AWS::Serverless::StateMachine
    Properties:
      Name: !Join ['-', [!Ref Prefix, 'dataproc', !Select [7, !Split ['-', !Ref AWS::StackId]]]]
      Definition:
        Comment: A description of my state machine
        StartAt: Set Example Runtime Properties
        States:
          Set Example Runtime Properties:
            Type: Pass
            Next: S3 Inventory Partition Step
            Result:
              inventory:
                bucket: !Ref SourceS3Bucket
                key: inventory/manifest.json
                output_prefix: inventory/temp/
              workshop_variables:
                output_bucket: !Ref DestinationS3Bucket
                output_prefix: output-data
                batch_output_files: 'yes'
                input_sampling: 2
                output_rows_per_file: !Ref BatchSize
          S3 Inventory Partition Step:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName: !Join [':', [!GetAtt InventoryPartitionLambda.Arn, '$LATEST']]
              Payload.$: "$"
            Retry:
            - ErrorEquals:
              - Lambda.ServiceException
              - Lambda.AWSLambdaException
              - Lambda.SdkClientException
              - Lambda.TooManyRequestsException
              IntervalSeconds: 2
              MaxAttempts: 6
              BackoffRate: 2
            Next: Scale Out Workers
            ResultPath: "$.stepresult"
            ResultSelector:
              body.$: "$.Payload.body"
          Scale Out Workers:
            Type: Task
            Next: Inline Map Orchestration
            Parameters:
              Service: !Ref MainService
              Cluster: !Ref MainECSCluster
              DesiredCount: 1000
            Resource: arn:aws:states:::aws-sdk:ecs:updateService
            ResultPath: null
          Inline Map Orchestration:
            Type: Map
            ItemProcessor:
              ProcessorConfig:
                Mode: INLINE
              StartAt: Distributed Map Data Processing
              States:
                Distributed Map Data Processing:
                  Type: Map
                  ItemProcessor:
                    ProcessorConfig:
                      Mode: DISTRIBUTED
                      ExecutionType: STANDARD
                    StartAt: Step Functions Run Activity
                    States:
                      Step Functions Run Activity:
                        Type: Task
                        Resource: !Ref MainSFNActivity
                        TimeoutSeconds: 900
                        End: true
                        Retry:
                        - ErrorEquals:
                          - States.TaskFailed
                          - States.Timeout
                          - 'An error occurred (SlowDown) when calling the PutObject operation (reached
                            max retries: 4): Please reduce your request rate.'
                          BackoffRate: 2
                          IntervalSeconds: 30
                          MaxAttempts: 3
                        HeartbeatSeconds: 300
                  ItemReader:
                    Resource: arn:aws:states:::s3:getObject
                    ReaderConfig:
                      InputType: MANIFEST
                      MaxItems: 0
                    Parameters:
                      Bucket.$: "$.key.bucket"
                      Key.$: "$.key.key"
                  MaxConcurrency: 1000
                  Label: S3objectkeys
                  ItemBatcher:
                    BatchInput:
                      workshop_variables.$: $.workshop_variables
                    MaxItemsPerBatch: 100
                  ResultWriter:
                    Resource: arn:aws:states:::s3:putObject
                    Parameters:
                      Bucket: !Ref DestinationS3Bucket
                      Prefix: !Sub "${Prefix}-dmap-results"
                  End: true
            Next: Destroy Workers
            ItemsPath: "$.stepresult.body.files"
            MaxConcurrency: 2
            ItemSelector:
              workshop_variables.$: "$.workshop_variables"
              key.$: "$$.Map.Item.Value"
          Destroy Workers:
            Type: Task
            Parameters:
              Service: !Ref MainService
              Cluster: !Ref MainECSCluster
              DesiredCount: 0
            Resource: arn:aws:states:::aws-sdk:ecs:updateService
            End: true
      Role: !GetAtt StepFunctionsRole.Arn
      Tags:
        Name: !Join ['-', [!Ref Prefix, 'dataproc', !Select [7, !Split ['-', !Ref AWS::StackId]]]]